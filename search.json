[{"title":"四月是你的谎言","url":"%2F2020%2F03%2F31%2F%E5%BF%83%E6%83%85.html","content":"<p class=\"description\"></p>\n\n<img src=\"https://\" alt=\"\" style=\"width:100%\" />\n\n\n四月如初\n\n人生多艰，风雨无数，幸亏一切还好。\n\n年后的天气，多是阴晴难测，多阴多雨。身后的一大驴子的好山好水也因为这特殊的时期少了游人的眷顾。我于窗前临盼着哪天的大好时光，在翠绿的光阴下狠狠的消遣一把。\n<!-- more -->\n常年的怡笑怒骂，使我这性子总是不惧不惊。深山探花，酒醉寻良人，夜深敲代码，这世间无论怎样，都会寻得所爱之事，恰恰是众生万物灵魂的相通，我在这滚滚红尘的浪涛中，怡然自得，冰打不住。\n\n疫情闹腾了4月有余，等待成了信仰。那些想见的人，有的不在了，有的因足不出户成了奢望。有些人，为了使命，奔赴于没有硝烟的战场，死生不问，无声无息。有些人，静守在光阴的屋檐，等待行将而至的春暖花开。然而春不会不来，如期而至，我国之盛事，一切惶恐不安焦虑也都是过眼云烟，最后还是山海平静，云白天蓝，桥上升烟，脚下踩钱。一切的一切终会过去，头发会脱落亦会再次重新乌黑。\n\n人这一生，总有些磨难，世间多苦，劫难虽多，又有何惧，大笑处置，过后还不是闲谈野话，樵夫饭茶，你我闲话。\n\n光阴有限，寸阴可贵，一刻千金。让我更懂得珍惜，以往漫不经心，闲云野鹤的我，也需要懂得感恩，懂得忍耐，学的努力，在最后的余晖里，在这艳阳的光阴下，在这杂乱不堪的曹舍里，在这红灯绿酒的小路上，好好地拼搏一下。等待行将而至的夏，送别即将离开的春。\n\n时疫年慌诸业空，万众隔离各西东。豪拿余阴做良褚，翱于九天飞化龙。\n \n\n<hr />\n","tags":["心情，杂记"],"categories":["心情"]},{"title":"mq面试","url":"%2F2020%2F03%2F31%2F%E9%9D%A2%E8%AF%95%2Fmq%E4%B8%AD%E9%97%B4%E4%BB%B6%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E3%80%82%E3%80%82%EF%BC%89.html","content":"<p class=\"description\"></p>\n\n<img src=\"https://\" alt=\"\" style=\"width:100%\" />\n\n\n# mq中间件面试题（持续更新。。）\n\n> 曾经走不出的日子，现在只剩下回忆了\n\n1. 各种mq的区别，以及mq的优点\n<!-- more -->\n解耦 削峰 异步\n\n中间件有点:将消息引入消息队列，非必要的逻辑使用mq，加快相应速度\n\n缺点：系统的可用性降低，系统的依赖越多，越容易挂掉\n\n硬生生加个mq进来，你怎么保证消息不被重复消费，怎么处理消息丢失的情况？等等问题\n\n| 特性        | ActiveMQ | RabbitMQ | RocketMQ | kafka    |\n| ----------- | -------- | -------- | -------- | -------- |\n| 开发语言    | java     | erlang   | java     | scala    |\n| 吞吐量（1） | 万级     | 万级     | 10万级   | 10万级   |\n| 时效性      | ms       | us级     | ms级     | ms级以内 |\n| 可靠性      | 较低     | 基本不丢 | 可0      | 可0      |\n| 可用性      | 高       | 高       | 非常高   | 非常高   |\n\n2. 如何保证消息不被重复消费？\n\n什么会造成重复消费?\n其实无论是那种消息队列，造成重复消费原因其实都是类似的。正常情况下，消费者在消费消息时候，消费完毕后，会发送一个确认信息给消息队列，消息队列就知道该消息被消费了，就会将该消息从消息队列中删除。只是不同的消息队列发送的确认信息形式不同,例如RabbitMQ是发送一个ACK确认消息，RocketMQ是返回一个CONSUME_SUCCESS成功标志，kafka实际上有个offset的概念，简单说一下(如果还不懂，出门找一个kafka入门到精通教程),就是每一个消息都有一个offset，kafka消费过消息后，需要提交offset，让消息队列知道自己已经消费过了。那造成重复消费的原因?，就是因为网络传输等等故障，确认信息没有传送到消息队列，导致消息队列不知道自己已经消费过该消息了，再次将该消息分发给其他的消费者。\n  如何解决?这个问题针对业务场景来答分以下几点\n  (1)比如，你拿到这个消息做数据库的insert操作。那就容易了，给这个消息做一个唯一主键，那么就算出现重复消费的情况，就会导致主键冲突，避免数据库出现脏数据。（还是选你吧）\n  (2)再比如，你拿到这个消息做redis的set的操作，那就容易了，不用解决，因为你无论set几次结果都是一样的，set操作本来就算幂等操作。\n  (3)如果上面两种情况还不行，上大招。准备一个第三方介质,来做消费记录。以redis为例，给消息分配一个全局id，只要消费过该消息，将<id,message>以K-V形式写入redis。那消费者开始消费前，先去redis中查询有没消费记录即可。\n\n3.如何保证消息的可靠传输？\n\n三个情况，生产者丢数据，消费者丢，通道丢\n\nrabbitMq的生产者有事务和confirm（常用），事务慢，confirm是ack机制\n\n队列丢一般是开启持久化 durable设为true \n\n3)消费者丢数据\n消费者丢数据一般是因为采用了自动确认消息模式。这种模式下，消费者会自动确认收到信息。这时rahbitMQ会立即将消息删除，这种情况下如果消费者出现异常而没能处理该消息，就会丢失该消息。\n至于解决方案，采用手动确认消息即可。\nkafka\nProducer在发布消息到某个Partition时，先通过ZooKeeper找到该Partition的Leader，然后无论该Topic的Replication Factor为多少（也即该Partition有多少个Replica），Producer只将该消息发送到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader中pull数据。\n针对上述情况，得出如下分析\n(1)生产者丢数据\n在kafka生产中，基本都有一个leader和多个follwer。follwer会去同步leader的信息。因此，为了避免生产者丢数据，做如下两点配置\n\n第一个配置要在producer端设置acks=all。这个配置保证了，follwer同步完成后，才认为消息发送成功。\n在producer端设置retries=MAX，一旦写入失败，这无限重试\n(2)消息队列丢数据\n针对消息队列丢数据的情况，无外乎就是，数据还没同步，leader就挂了，这时zookpeer会将其他的follwer切换为leader,那数据就丢失了。针对这种情况，应该做两个配置。\n\nreplication.factor参数，这个值必须大于1，即要求每个partition必须有至少2个副本\nmin.insync.replicas参数，这个值必须大于1，这个是要求一个leader至少感知到有至少一个follower还跟自己保持联系\n这两个配置加上上面生产者的配置联合起来用，基本可确保kafka不丢数据\n\n(3)消费者丢数据\n这种情况一般是自动提交了offset，然后你处理程序过程中挂了。kafka以为你处理好了。再强调一次offset是干嘛的\noffset：指的是kafka的topic中的每个消费组消费的下标。简单的来说就是一条消息对应一个offset下标，每次消费数据的时候如果提交offset，那么下次消费就会从提交的offset加一那里开始消费。\n比如一个topic中有100条数据，我消费了50条并且提交了，那么此时的kafka服务端记录提交的offset就是49(offset从0开始)，那么下次消费的时候offset就从50开始消费。\n解决方案也很简单，改成手动提交即可。\n##\n\n##\n\n##\n\n<hr />\n\n\n\n\n","tags":["mq，面试"],"categories":["面试"]},{"title":"reids面试题","url":"%2F2020%2F03%2F31%2F%E9%9D%A2%E8%AF%95%2F%E8%BF%99%E6%98%AF%E7%AF%87%E9%95%BF%E6%96%87.html","content":"<p class=\"description\"></p>\n\n<img src=\"https://\" alt=\"\" style=\"width:100%\" />\n# 这是篇长文\n\n> 有一种落差是，你配不上自己的野心，也辜负了所有的苦难\n\n##1. redis支持哪几种数据类型\n\n1）String 最基本的数据类型\n\n2）list：按照添加顺序的字符串列表\n\n3）set：无序的字符串集合\n\n4）sorted set 有序的字符串集合、\n\n5）hash 类似与map key-value\n\n## 2.reids持久化机制AOF与RDB\n<!-- more -->\n```js\n打开AOF持久化机制之后，redis每次接收到一条写命令，就会写入日志文件中，当然是先写入os cache的，然后每隔一定时间再fsync一下\n\n而且即使AOF和RDB都开启了，redis重启的时候，`也是优先通过AOF进行数据恢复的，因为aof数据比较完整`\n\n可以配置AOF的fsync策略，有三种策略可以选择，一种是每次写入一条数据就执行一次fsync; 一种是每隔一秒执行一次fsync; 一种是不主动执行fsync\n\nalways -> 每次写入一条数据，立即将这个数据对应的写日志fsync到磁盘上去，性能非常非常差，吞吐量很低; \nmysql -> 内存策略，大量磁盘，QPS到多少，一两k。QPS，每秒钟的请求数量\nredis -> 内存，磁盘持久化，QPS到多少，单机，一般来说，上万QPS没问题\n`everysec`: 每秒将os cache中的数据fsync到磁盘，这个最常用的，生产环境一般都这么配置，性能很高，QPS还是可以上万的\nno: 仅仅redis负责将数据写入os cache就撒手不管了，然后后面os自己会时不时有自己的策略将数据刷入磁盘，不可控了\naof rewrite配置策略\nauto-aof-rewrite-percentage 100\nauto-aof-rewrite-min-size 64mb\n说明：如果上一次aof是128mb ，这次的如果超过了上次的100%且大于64mb就会执行rewrite\n（1）redis fork一个子进程\n（2）子进程基于当前内存中的数据，构建日志，开始往一个新的临时的AOF文件中写入日志\n（3）redis主进程，接收到client新的写操作之后，在内存中写入日志，同时新的日志也继续写入旧的AOF文件\n（4）子进程写完新的日志文件之后，redis主进程将内存中的新日志再次追加到新的AOF文件中\n（5）用新的日志文件替换掉旧的日志文件\n1.rdb优点：\n（1）RDB会生成多个数据文件，每个数据文件都代表了某一个时刻中redis的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说Amazon的S3云服务上去，在国内可以是阿里云的ODPS分布式存储上，以预定好的备份策略来定期备份redis中的数据\n（2）RDB对redis对外提供的读写服务，影响非常小，可以让redis保持高性能，因为redis主进程只需要fork一个子进程，让子进程执行磁盘IO操作来进行RDB持久化即可\n（3）相对于AOF持久化机制来说，直接基于RDB数据文件来重启和恢复redis进程，更加快速\n2.rdb缺点\n（1）如果想要在redis故障时，尽可能少的丢失数据，那么RDB没有AOF好。一般来说，RDB数据快照文件，都是每隔5分钟，或者更长时间生成一次，这个时候就得接受一旦redis进程宕机，那么会丢失最近5分钟的数据\n（2）RDB每次在fork子进程来执行RDB快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒\n3.aof优点\n（1）AOF可以更好的保护数据不丢失，一般AOF会每隔1秒，通过一个后台线程执行一次fsync操作，最多丢失1秒钟的数据\n（2）AOF日志文件以append-only模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复\n（3）AOF日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在rewrite log的时候，会对其中的指导进行压缩，创建出一份需要恢复数据的最小日志出来。再创建新日志文件的时候，老的日志文件还是照常写入。当新的merge后的日志文件ready的时候，再交换新老日志文件即可。\n（4）AOF日志文件的命令通过非常可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据\n4.aof缺点\n（1）对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大\n（2）AOF开启后，支持的写QPS会比RDB支持的写QPS低，因为AOF一般会配置成每秒fsync一次日志文件，当然，每秒一次fsync，性能也还是很高\n（3）以前AOF发生过bug，就是通过AOF记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似AOF这种较为复杂的基于命令日志/merge/回放的方式，比基于RDB每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有bug。不过AOF就是为了避免rewrite过程导致的bug，因此每次rewrite并不是基于旧的指令日志进行merge的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。\n5.选择\n（1）两种都要有\n（2）第一，通过AOF做冷备，没有RDB做冷备，来的恢复速度更快; 第二，RDB每次简单粗暴生成数据快照，更加健壮，可以避免AOF这种复杂的备份和恢复机制的bug\n（3）用RDB来做不同程度的冷备，在AOF文件都丢失或损坏不可用的时候，还可以使用RDB来进行快速的数据恢复\n```\n\n ## 3.缓存雪崩、缓存穿透、缓存预热、缓存更新、缓存降级等问题\n\n**缓存雪崩**\n\n缓存在同一时间内大量键过期（失效），接着来的一大波请求瞬间都落在了数据库中导致连接异常。\n\n解决方案：\n\n方案1、也是像解决缓存穿透一样加锁排队;\n\n方案2、建立备份缓存，缓存A和缓存B，A设置超时时间，B不设值超时时间，先从A读缓存，A没有读B，并且更新A缓存和B缓存;\n\n方案3、设置缓存超时时间的时候加上一个随机的时间长度，比如这个缓存key的超时时间是固定的5分钟加上随机的2分钟，酱紫可从一定程度上避免雪崩问题\n\n互斥锁排队：\n\n即根据key获取value值为空时，锁上，从数据库中load数据后再释放锁。若其它线程获取锁失败，则等待一段时间后重试。这里要注意，分布式环境中要使用分布式锁，单机的话用普通的锁（synchronized、Lock）就够了\n\n```java\npublic String getWithLock(String key, Jedis jedis, String lockKey, String uniqueId, long expireTime) {\n    // 通过key获取value\n    String value = redisService.get(key);\n    if (StringUtil.isEmpty(value)) {\n        //封装的tryDistributedLock包括setnx和expire两个功能，在低版本的redis中不支持\n        try {\n            boolean locked = redisService.tryDistributedLock(jedis, lockKey, uniqueId, expireTime);\n            if (locked) {\n                value = userService.getById(key);\n                redisService.set(key, value);\n                redisService.del(lockKey);\n                return value;\n            } else {\n                // 其它线程进来了没获取到锁便等待50ms后重试\n                Thread.sleep(50);\n                getWithLock(key, jedis, lockKey, uniqueId, expireTime);\n            }\n        } catch (Exception e) {\n            log.error(\"getWithLock exception=\" + e);\n            return value;\n        } finally {\n            redisService.releaseDistributedLock(jedis, lockKey, uniqueId);\n        }\n    }\n    return value;\n}\n```\n\n设置过期时间\n\n```java\npublic String getByKey(String keyA,String keyB) {\n    String value = redisService.get(keyA);\n    if (StringUtil.isEmpty(value)) {\n        value = redisService.get(keyB);\n        String newValue = getFromDbById();\n        redisService.set(keyA,newValue,31, TimeUnit.DAYS);\n        redisService.set(keyB,newValue);\n    }\n    return value;\n}\n```\n\n**缓存穿透**\n\n缓存击穿表示恶意用户模拟请求很多缓存中不存在的数据，由于缓存中都没有，导致这些请求短时间内直接落在了数据库上，导致数据库异常。这个我们在实际项目就遇到了，有些抢购活动、秒杀活动的接口API被大量的恶意用户刷，导致短时间内数据库c超时了，好在数据库是读写分离，同时也有进行接口限流，hold住了。\n\n**方案1、使用互斥锁排队**\n\n**方案2、接口限流与熔断、降级**\n\n**方案3、布隆过滤器**\n\nbloomfilter就类似于一个hash set，用于快速判某个元素是否存在于集合中，其典型的应用场景就是快速判断一个key是否存在于某容器，不存在就直接返回。布隆过滤器的关键就在于hash算法和容器大小，下面先来简单的实现下看看效果，我这里用guava实现的布隆过滤器\n\n```java\npublic class BloomFilterTest {\n \n    private static final int capacity = 1000000;\n    private static final int key = 999998;\n \n    private static BloomFilter<Integer> bloomFilter = BloomFilter.create(Funnels.integerFunnel(), capacity);\n \n    static {\n        for (int i = 0; i < capacity; i++) {\n            bloomFilter.put(i);\n        }\n    }\n \n    public static void main(String[] args) {\n        /*返回计算机最精确的时间，单位微妙*/\n        long start = System.nanoTime();\n \n        if (bloomFilter.mightContain(key)) {\n            System.out.println(\"成功过滤到\" + key);\n        }\n        long end = System.nanoTime();\n        System.out.println(\"布隆过滤器消耗时间:\" + (end - start));\n        int sum = 0;\n        for (int i = capacity + 20000; i < capacity + 30000; i++) {\n            if (bloomFilter.mightContain(i)) {\n                sum = sum + 1;\n            }\n        }\n        System.out.println(\"错判率为:\" + sum);\n    }\n}\n```\n\n分布式锁：分布式锁 使用zk，使用简单，可靠性高 ，redis的并发性好\n\n```java\npackage com.roncoo.eshop.cache.zk;\n\nimport java.util.concurrent.CountDownLatch;\n\nimport org.apache.zookeeper.CreateMode;\nimport org.apache.zookeeper.WatchedEvent;\nimport org.apache.zookeeper.Watcher;\nimport org.apache.zookeeper.Watcher.Event.KeeperState;\nimport org.apache.zookeeper.ZooDefs.Ids;\nimport org.apache.zookeeper.ZooKeeper;\n\n/**\n * ZooKeeperSession\n * @author Administrator\n *\n */\npublic class ZooKeeperSession {\n\t\n\tprivate static CountDownLatch connectedSemaphore = new CountDownLatch(1);\n\t\n\tprivate ZooKeeper zookeeper;\n\n\tpublic ZooKeeperSession() {\n\t\t// 去连接zookeeper server，创建会话的时候，是异步去进行的\n\t\t// 所以要给一个监听器，说告诉我们什么时候才是真正完成了跟zk server的连接\n\t\ttry {\n\t\t\tthis.zookeeper = new ZooKeeper(\n\t\t\t\t\t\"192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181\", \n\t\t\t\t\t50000, \n\t\t\t\t\tnew ZooKeeperWatcher());\n\t\t\t// 给一个状态CONNECTING，连接中\n\t\t\tSystem.out.println(zookeeper.getState());\n\t\t\t\n\t\t\ttry {\n\t\t\t\t// CountDownLatch\n\t\t\t\t// java多线程并发同步的一个工具类\n\t\t\t\t// 会传递进去一些数字，比如说1,2 ，3 都可以\n\t\t\t\t// 然后await()，如果数字不是0，那么久卡住，等待\n\t\t\t\t\n\t\t\t\t// 其他的线程可以调用coutnDown()，减1\n\t\t\t\t// 如果数字减到0，那么之前所有在await的线程，都会逃出阻塞的状态\n\t\t\t\t// 继续向下运行\n\t\t\t\t\n\t\t\t\tconnectedSemaphore.await();\n\t\t\t} catch(InterruptedException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\n\t\t\tSystem.out.println(\"ZooKeeper session established......\");\n\t\t} catch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n\t\n\t/**\n\t * 获取分布式锁\n\t * @param productId\n\t */\n\tpublic void acquireDistributedLock(Long productId) {\n\t\tString path = \"/product-lock-\" + productId;\n\t\n\t\ttry {\n\t\t\tzookeeper.create(path, \"\".getBytes(), \n\t\t\t\t\tIds.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);//临时节点\n\t\t\tSystem.out.println(\"success to acquire lock for product[id=\" + productId + \"]\");  \n\t\t} catch (Exception e) {\n\t\t\t// 如果那个商品对应的锁的node，已经存在了，就是已经被别人加锁了，那么就这里就会报错\n            //并尝试持续获取锁，指到拿到\n\t\t\t// NodeExistsException\n\t\t\tint count = 0;\n\t\t\twhile(true) {\n\t\t\t\ttry {\n\t\t\t\t\tThread.sleep(1000); \n\t\t\t\t\tzookeeper.create(path, \"\".getBytes(), \n\t\t\t\t\t\t\tIds.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);\n\t\t\t\t} catch (Exception e2) {\n\t\t\t\t\tcount++;\n\t\t\t\t\tSystem.out.println(\"the \" + count + \" times try to acquire lock for product[id=\" + productId + \"]......\");\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tSystem.out.println(\"success to acquire lock for product[id=\" + productId + \"] after \" + count + \" times try......\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\t\n\t/**\n\t * 释放掉一个分布式锁\n\t * @param productId\n\t */\n\tpublic void releaseDistributedLock(Long productId) {\n\t\tString path = \"/product-lock-\" + productId;\n\t\ttry {\n\t\t\tzookeeper.delete(path, -1); \n\t\t\tSystem.out.println(\"release the lock for product[id=\" + productId + \"]......\");  \n\t\t} catch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n\t\n\t/**\n\t * 建立zk session的watcher\n\t * @author Administrator\n\t *\n\t */\n\tprivate class ZooKeeperWatcher implements Watcher {\n\n\t\tpublic void process(WatchedEvent event) {\n\t\t\tSystem.out.println(\"Receive watched event: \" + event.getState());\n\t\t\tif(KeeperState.SyncConnected == event.getState()) {\n\t\t\t\tconnectedSemaphore.countDown();\n\t\t\t} \n\t\t}\n\t\t\n\t}\n\t\n\t/**\n\t * 封装单例的静态内部类\n\t * @author Administrator\n\t *\n\t */\n\tprivate static class Singleton {\n\t\t\n\t\tprivate static ZooKeeperSession instance;\n\t\t\n\t\tstatic {\n\t\t\tinstance = new ZooKeeperSession();\n\t\t}\n\t\t\n\t\tpublic static ZooKeeperSession getInstance() {\n\t\t\treturn instance;\n\t\t}\n\t\t\n\t}\n\t\n\t/**\n\t * 获取单例\n\t * @return\n\t */\n\tpublic static ZooKeeperSession getInstance() {\n\t\treturn Singleton.getInstance();\n\t}\n\t\n\t/**\n\t * 初始化单例的便捷方法\n\t */\n\tpublic static void init() {\n\t\tgetInstance();\n\t}\n\t\n}\n\n```\n\n**缓存预热**\n\n缓存，热，预热，热数据\n\n解决方案：引入大数据实时计算storm\n\n## **4.redis 内存淘汰机制。**\n\n\n\nredis 内存淘汰机制（MySQL里有2000w数据，Redis中只存20w的数据，如何保证Redis中的数据都是热点数据？）\n\n**volatile-lru**：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰\n\n**volatile-ttl**：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰\n\n**volatile-random**：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰\n\n**allkeys-lru**：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key（这个是最常用的）.\n\n**allkeys-random**：从数据集（server.db[i].dict）中任意选择数据淘汰\n\n**no-eviction**：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！\n\n##5.如何解决 Redis 的并发竞争 Key 问题\n\n所谓 Redis 的并发竞争 Key 的问题也就是多个系统同时对一个 key 进行操作，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同！\n\n推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（如果不存在 Redis 的并发竞争 Key 问题，不要使用分布式锁，这样会影响性能）\n\n基于zookeeper临时有序节点可以实现的分布式锁。大致思想为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。\n\n锁上面已经提供啦\n\n##6.如何保证缓存与数据库双写时的数据一致性？\n\n你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？\n\n一般来说，就是如果你的系统不是严格要求缓存+数据库必须一致性的话，缓存可以稍微的跟数据库偶尔有不一致的情况，最好不要做这个方案，读请求和写请求串行化，串到一个内存队列里去，这样就可以保证一定不会出现不一致的情况\n\n串行化之后，就会导致系统的吞吐量会大度的降低，用比正常情况下多几倍的机器去支撑线上的一个请求。\n\n还有一种方式就是可能会暂时产生不一致的情况，但是发生的几率特别小，就是**先更新数据库，然后再删除缓存。**\n\n**这种情况不存在并发问题么？**\n\n不是的。假设这会有两个请求，一个请求A做查询操作，一个请求B做更新操作，那么会有如下情形产生\n\n（1）缓存刚好失效\n （2）请求A查询数据库，得一个旧值\n （3）请求B将新值写入数据库\n （4）请求B删除缓存\n （5）请求A将查到的旧值写入缓存\n\n 几率很小啊，数据库读明显的比写快\n\n**如何解决上述并发问题？**\n\n首先，给缓存设有效时间是一种方案。其次，采用异步延时删除策略，保证读请求完成以后，再进行删除操作。\n\n可以用mq，有数据写的情况下进队列发消息加锁更新cache和redis\n\n## **7.redis为什么是单线程的**\n\n1. 大部分请求是纯粹的内存操作（非常快速）\n2. 采用单线程，避免了不必要的上下文切换和竞争条件\n3. 非阻塞 IO，内部采用 epoll，epoll 中的读、写、关闭、连接都转化成了事件，然后利用 epoll 的多路复用特性，避免 IO 代价。\n\n## 8.聊一聊redis的数据分布算法\n\n有hash，一致性hash hash solt\n\n- ash 算法\n   按照key的hash值对节点进行取模，然后得到的结果就是存放数据的节点；\n   缺点：如果一个节点挂了，新的数据按照新的节点数据取模导致无法取到数据（所有数据都取不到），于是就会被击穿到数据库，导致数据库压力增大。同时需要重新进行大量计算，把原有数据进行重新分配。\n\n- 一致性hash算法（虚拟节点算法）\n   把节点分布到一个圆环中分布，key的hash值于当前节点hash值比较，然后确定在圆环上的位置，同时按照顺时针方向去寻找最近的一个节点。\n   按照上述理论，当一个节点失效的时候，就只有这个节点对应的数据无法取到，而其他节点数据依然还是可以取到。所以，可用性比hash算法要高。\n   缺点：针对热点数据问题，可能都会涌入到同一个节点中，而其他节点数据又少。\n   优化：在圆环中增加多个虚拟的同类节点，比如原来有1号 2 号 3号 这三个节点，现在虚拟出多个1号、2好、3号的节点分布在圆环上，这样就可以更细粒度的切割圆环数据，尽量减少了热点数据集中问题。\n\n- hash slot 算法\n   上述都是物理节点的分布做算法，而redis cluster是通过在节点中分布虚拟的小节点来分布数据。redis cluster会在整个集群节点中分布 16384 个虚拟slot。每个节点分配一部分slot。然后所有的key的hash对 16384 取模，从而知道在哪个slot，也就知道了数据在哪个节点上。 当一个节点挂了时，就会把这个节点上的slot迁移到其他节点上。\n   因此，可以认为hash slot 算法，是一致性hash算法的优化。\n   优点：即拥有了一致性hash算法的优点，又避免了热点数据；\n   master的增加和移除也是非常方面，增加master，就把其他节点的slot移动到这个master。移除master，就把当前master的slot移动到其他节点。\n\n在罗列可能被问到的\n\n## 9.redis 和 memcached 的区别\n\nredis支持更丰富的数据类型（支持更复杂的应用场景）：Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。memcache支持简单的数据类型，String。\nRedis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而Memecache把数据全部存在内存之中。\n集群模式：memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 redis 目前是原生支持 cluster 模式的.\nMemcached是多线程，非阻塞IO复用的网络模型；Redis使用单线程的多路 IO 复用模型。\n\n这里科普下：redis的作者叫antirez ，开源领域的大牛，人家自己写自己公司用的，开源后牛逼了，几十个国家再用，然后利用业余时间维护，后来被vmware公司赞助还是从事redis的开发，最后公司都被VMware收购啦。\n\n喜欢她的一句话，人家都40的老娘们了，还写着呢\n\n> 在技术上把时间花在讨论性别问题实在是浪费时间，还不如老老实实去编软件来的有价值\n\n用很流行的一句话就说`Talk is cheap.Show me the code`\n\n##10. redis集群原理\n\nRedis Sentinal着眼于高可用，在master宕机时会自动将slave提升为master，继续提供服务。\n\nRedis Cluster着眼于扩展性，在单个redis内存不足时，使用Cluster进行分片存储。\n\n这里不会将太细：\n\nsentinal 中文叫森町弄就是哨兵\n\n主要功能\n\n（1）集群监控，负责监控redis master和slave进程是否正常工作\n（2）消息通知，如果某个redis实例有故障，那么哨兵负责发送消息作为报警通知给管理员\n（3）故障转移，如果master node挂掉了，会自动转移到slave node上\n（4）配置中心，如果故障转移发生了，通知client客户端新的master地址\n\n哨兵之间互相监控协同合作\n\n这里说到故障转移的选举策略：\n\n（1）哨兵至少需要3个实例，来保证自己的健壮性 因为1+1>1 这种\n（2）哨兵 + redis主从的部署架构，是不会保证数据零丢失的，只能保证redis集群的高可用性\n（3）对于哨兵 + redis主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练\n\nm1 S1\n\nM2 S2\n\nM3 S3\n\nConfiguration: quorum = 2，majority\n\n如果M1所在机器宕机了，那么三个哨兵还剩下2个，S2和S3可以一致认为master宕机，然后选举出一个来执行故障转移\n\n同时3个哨兵的majority是2，所以还剩下的2个哨兵运行着，就可以允许执行故障转移\n\nsdown odown客观宕机，与主观宕机\n\nsdown是主观宕机，就一个哨兵如果自己觉得一个master宕机了，那么就是主观宕机\n\n就是ping master（同时sentinal之间也来回交谈）\n\n超过了is-master-down-after-milliseconds指定的毫秒数之后，就主观认为master宕机\n\nodown是客观宕机，如果quorum数量的哨兵都觉得一个master宕机了，那么就是客观宕机\n\n如果一个哨兵在指定时间内，收到了quorum指定数量的其他哨兵也认为那个master是sdown了，那么就认为是odown了，客观认为master宕机\n\nslave=>master\n\n（1）跟master断开连接的时长，越长越没机会 (down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state\n（2）slave优先级 越低，优先级越高\n（3）复制offset 越多约有机会\n（4）run id 每个都有很长的字符串\n\n排序：\n\n（1）按照slave优先级进行排序，slave priority越低，优先级就越高\n（2）如果slave priority相同，那么看replica offset，哪个slave复制了越多的数据，offset越靠后，优先级就越高\n（3）如果上面两个条件都相同，那么选择一个run id比较小的那个slave\n\nquorum和majority\n\n每次一个哨兵要做主备切换，首先需要quorum数量的哨兵认为odown，然后选举出一个哨兵来做切换，这个哨兵还得得到majority哨兵的授权，才能正式执行切换\n\n如果quorum < majority，比如5个哨兵，majority就是3，quorum设置为2，那么就3个哨兵授权就可以执行切换\n\n但是如果quorum >= majority，那么必须quorum数量的哨兵都授权，比如5个哨兵，quorum是5，那么必须5个哨兵都同意授权，才能执行切换\n\nquorum理解成有权利家庭成员，比如bb和mm，majority是大多数人\n\nredis cluster 这个可扩容性强，可实现高吞吐\n\n用ruby部署这里说下自己部署的时候会有很多坑，ruby版本问题包问题，尽量用下面的，新版redis还好\n\n```\nyum install -y ruby\nyum install -y rubygems\ngem install redis\n将单机redis/src/redis-trib.rb 复制到。usr/bin\nredis-trib.rb create --repilcas 1 ...... --repilcas 是master的副本数自动指定的 solt自动hash分配\n添加node redis-trib.rb add-node --slave --master-id\n不加后面参数就是master\n删除redis-trib.rb del-node  \n删的是maser的话slave会自动挂接其他master\n\n```\n\n","tags":["mq，面试"],"categories":["面试"]},{"title":"一个推荐","url":"%2F2020%2F03%2F29%2F%E4%B8%80%E4%B8%AA%E6%8E%A8%E8%8D%90.html","content":"# 一个推荐\n\n## 没了这个我好像不会上网了\n\n> 谁说过一件难过的事情最多持续几天，因为后面会有更难的事\n\n最近因为pandownload的维护很多资料都不太好找，无意间发现了一个很牛逼的插件\n\n`Tampermonkey` 俗称‘油猴’，是一款浏览器插件管理器。可以安装一些网友开发的强大的浏览器插件。\n\n举例：全网音乐下载播放，全网视频，百度去光告，自定义页面，百度云增加全网盘搜索功能，不过他的搜索引擎貌似没有度盘搜好用。\n\n1. 插件安装[百度云提取码：ejzd](https://pan.baidu.com/s/1znXdqdQ5-LepdB9gE6CPgw )\n\n谷歌浏览器 设置 =>扩展程序 文件拖进去即可\n\n2. 插件安装\n\n[点我查找](https://greasyfork.org/zh-CN)，在这个网站寻找需要的就好啦，安装即生效很方便\n\n3. 推荐 \n\n   屏幕保护，百度云，vip音乐，vip视频解析，去广告\n\n4. 我安装的图\n\n![image-20200329173353526](一个推荐/wallhaven-248751.png)"},{"title":"那些实战的演练","url":"%2F2020%2F03%2F29%2F%E9%82%A3%E4%BA%9B%E5%AE%9E%E6%88%98%E7%9A%84%E6%BC%94%E7%BB%83.html","content":"# nginx 那些实战的演练\n\n> 转身的寥寥笑脸，不甘的甘愿\n\n## 1.架构设置\n\n流量分发：在分发层的nginx实施\n\n多级缓存架构的控制逻辑：在应用层的nginx实施\n\n热点数据的自动降级，也写在分发层\n\n1. 第一个nginx作为应用层\n\n（1）部署openresty\n\n```lua\nmkdir -p /usr/servers  \ncd /usr/servers/\n\nyum install -y readline-devel pcre-devel openssl-devel gcc\n\nwget http://openresty.org/download/ngx_openresty-1.7.7.2.tar.gz  \ntar -xzvf ngx_openresty-1.7.7.2.tar.gz  \ncd /usr/servers/ngx_openresty-1.7.7.2/\n\ncd bundle/LuaJIT-2.1-20150120/  \nmake clean && make && make install  \nln -sf luajit-2.1.0-alpha /usr/local/bin/luajit\n\ncd bundle  \nwget https://github.com/FRiCKLE/ngx_cache_purge/archive/2.3.tar.gz  \ntar -xvf 2.3.tar.gz  \n\ncd bundle  \nwget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v0.3.0.tar.gz  \ntar -xvf v0.3.0.tar.gz  \n\ncd /usr/servers/ngx_openresty-1.7.7.2  \n./configure --prefix=/usr/servers --with-http_realip_module  --with-pcre  --with-luajit --add-module=./bundle/ngx_cache_purge-2.3/ --add-module=./bundle/nginx_upstream_check_module-0.3.0/ -j2  \nmake && make install \n\ncd /usr/servers/  \nll\n\n/usr/servers/luajit\n/usr/servers/lualib\n/usr/servers/nginx\n/usr/servers/nginx/sbin/nginx -V \n```\n\n常用的设置\n\n```\n/usr/servers/nginx/sbin/nginx -t  #检测\n/usr/servers/nginx/sbin/nginx     #启动\n/usr/servers/nginx/sbin/nginx -s reload  #重启\ntail -f /usr/servers/nginx/logs/error.log #查看异常日志\n```\n\n代码\n\n```lua\n项目工程结构\n\nhello\n    hello.conf     \n    lua              \n      hello.lua\n    lualib            \n      *.lua\n      *.so\n\n放在/usr/hello目录下\n\n/usr/servers/nginx/conf/nginx.conf\n\nworker_processes  2;  \n\nerror_log  logs/error.log;  \n\nevents {  \n    worker_connections  1024;  \n}  \n\nhttp {  \n    include       mime.types;  \n    default_type  text/html;  \n  \n    lua_package_path \"/usr/hello/lualib/?.lua;;\";  \n    lua_package_cpath \"/usr/hello/lualib/?.so;;\"; \n    include /usr/hello/hello.conf;  \n}  \n\n/usr/hello/hello.conf\n\nserver {  \n    listen       80;  \n    server_name  _;  \n  \n    location /lua {  \n        default_type 'text/html';  \n        lua_code_cache off;  \n        content_by_lua_file /usr/example/lua/test.lua;  \n    }  \n}  \n\n```\n\n## 2.基于id的流量分发\n\n1、获取请求参数，比如productId\n2、对productId进行hash\n3、hash值对应用服务器数量取模，获取到一个应用服务器\n4、利用http发送请求到应用层nginx\n5、获取响应后返回\n\n首先引入lua http lib包\n\nwget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http_headers.lua  \nwget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http.lua \n\n代码：\n\n```\nlocal uri_args = ngx.req.get_uri_args()\nlocal productId = uri_args[\"productId\"]\n\nlocal host = {\"192.168.31.19\", \"192.168.31.187\"}\nlocal hash = ngx.crc32_long(productId)\nhash = (hash % 2) + 1  \nbackend = \"http://\"..host[hash]\n\nlocal method = uri_args[\"method\"]\nlocal requestBody = \"/\"..method..\"?productId=\"..productId\n\nlocal http = require(\"resty.http\")  \nlocal httpc = http.new()  \n\nlocal resp, err = httpc:request_uri(backend, {  \n    method = \"GET\",  \n    path = requestBody\n})\n\nif not resp then  \n    ngx.say(\"request error :\", err)  \n    return  \nend\n\nngx.say(resp.body)  \n  \nhttpc:close() \n```\n\n```lua\nlocal url_args = ngx.req.get_uri_args\nlocal productId = url_args[\"productId\"]\nlocal host =[1,2]\nlocal hash = ngx.crc32_long(productId)\nlocal index = hash%2 +1\nbackend = \"http://\"..host[index]\nlocal menthod = url_args[\"method\"]\nlocal requesBody = \"/\"..menthod..\"?productId=\"..productId\nlocal http = require(\"resty.http\")\nlocal httpc = http.new\nlcoal resq ,err =httpc:request_url(backed,{\n        menthod=\"GET\",\n        path=requestBody\n    })\nif not resq then\n    ngx.say(\"req error :\",err)\n    return\n    end\nngx.say(resq.body)\nhttpc:close()\n/usr/servers/nginx/sbin/nginx -s reload\n```\n\nnginx缓存\n\n```lua\n\n分发层nginx，lua应用，会将商品id，商品店铺id，都转发到后端的应用nginx\n\n/usr/servers/nginx/sbin/nginx -s reload\n\n1、应用nginx的lua脚本接收到请求\n\n2、获取请求参数中的商品id，以及商品店铺id\n\n3、根据商品id和商品店铺id，在nginx本地缓存中尝试获取数据\n\n4、如果在nginx本地缓存中没有获取到数据，那么就到redis分布式缓存中获取数据，如果获取到了数据，还要设置到nginx本地缓存中\n\n但是这里有个问题，建议不要用nginx+lua直接去获取redis数据\n\n因为openresty没有太好的redis cluster的支持包，所以建议是发送http请求到缓存数据生产服务，由该服务提供一个http接口\n\n缓存数生产服务可以基于redis cluster api从redis中直接获取数据，并返回给nginx\n\ncd /usr/hello/lualib/resty/  \nwget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http_headers.lua  \nwget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http.lua \n\n5、如果缓存数据生产服务没有在redis分布式缓存中没有获取到数据，那么就在自己本地ehcache中获取数据，返回数据给nginx，也要设置到nginx本地缓存中\n\n6、如果ehcache本地缓存都没有数据，那么就需要去原始的服务中拉去数据，该服务会从mysql中查询，拉去到数据之后，返回给nginx，并重新设置到ehcache和redis中\n\n这里先不考虑，后面要专门讲解一套分布式缓存重建并发冲突的问题和解决方案\n\n7、nginx最终利用获取到的数据，动态渲染网页模板\n\ncd /usr/hello/lualib/resty/\nwget https://raw.githubusercontent.com/bungle/lua-resty-template/master/lib/resty/template.lua\nmkdir /usr/hello/lualib/resty/html\ncd /usr/hello/lualib/resty/html\nwget https://raw.githubusercontent.com/bungle/lua-resty-template/master/lib/resty/template/html.lua\n\n在hello.conf的server中配置模板位置\n\nset $template_location \"/templates\";  \nset $template_root \"/usr/hello/templates\";\n\nmkdir /usr/hello/templates\n\nvi product.html\n\nproduct id: {* productId *}<br/>\nproduct name: {* productName *}<br/>\nproduct picture list: {* productPictureList *}<br/>\nproduct specification: {* productSpecification *}<br/>\nproduct service: {* productService *}<br/>\nproduct color: {* productColor *}<br/>\nproduct size: {* productSize *}<br/>\nshop id: {* shopId *}<br/>\nshop name: {* shopName *}<br/>\nshop level: {* shopLevel *}<br/>\nshop good cooment rate: {* shopGoodCommentRate *}<br/>\n\n8、将渲染后的网页模板作为http响应，返回给分发层nginx\n\nhello.conf中：\n\nlua_shared_dict my_cache 128m;\n\nlua脚本中：\n\nlocal uri_args = ngx.req.get_uri_args()\nlocal productId = uri_args[\"productId\"]\nlocal shopId = uri_args[\"shopId\"]\n\nlocal cache_ngx = ngx.shared.my_cache\n\nlocal productCacheKey = \"product_info_\"..productId\nlocal shopCacheKey = \"shop_info_\"..shopId\n\nlocal productCache = cache_ngx:get(productCacheKey)\nlocal shopCache = cache_ngx:get(shopCacheKey)\n\nif productCache == \"\" or productCache == nil then\n\tlocal http = require(\"resty.http\")\n\tlocal httpc = http.new()\n\n\tlocal resp, err = httpc:request_uri(\"http://192.168.31.179:8080\",{\n  \t\tmethod = \"GET\",\n  \t\tpath = \"/getProductInfo?productId=\"..productId\n\t})\n\n\tproductCache = resp.body\n\tcache_ngx:set(productCacheKey, productCache, 10 * 60)\nend\n\nif shopCache == \"\" or shopCache == nil then\n\tlocal http = require(\"resty.http\")\n\tlocal httpc = http.new()\n\n\tlocal resp, err = httpc:request_uri(\"http://192.168.31.179:8080\",{\n  \t\tmethod = \"GET\",\n  \t\tpath = \"/getShopInfo?shopId=\"..shopId\n\t})\n\n\tshopCache = resp.body\n\tcache_ngx:set(shopCacheKey, shopCache, 10 * 60)\nend\n\nlocal cjson = require(\"cjson\")\nlocal productCacheJSON = cjson.decode(productCache)\nlocal shopCacheJSON = cjson.decode(shopCache)\n\nlocal context = {\n\tproductId = productCacheJSON.id,\n\tproductName = productCacheJSON.name,\n\tproductPrice = productCacheJSON.price,\n\tproductPictureList = productCacheJSON.pictureList,\n\tproductSpecification = productCacheJSON.specification,\n\tproductService = productCacheJSON.service,\n\tproductColor = productCacheJSON.color,\n\tproductSize = productCacheJSON.size,\n\tshopId = shopCacheJSON.id,\n\tshopName = shopCacheJSON.name,\n\tshopLevel = shopCacheJSON.level,\n\tshopGoodCommentRate = shopCacheJSON.goodCommentRate\n}\n\nlocal template = require(\"resty.template\")\ntemplate.render(\"product.html\", context)\n\n```\n\n","tags":["nginx"],"categories":["nginx"]},{"title":"nginx学习笔记（2）","url":"%2F2020%2F03%2F29%2Fnginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89.html","content":"#`nginx`学习笔记  (二)\n\n> 时钟在走，时光飞逝，我失去了船，得到了宝藏，也失去了你。\n\n## 1. `openResty`安装\n\n==TODO\n\n## 2. hello_world\n\nnginx 如何嵌入 lua 脚本。方法就是在nginx的配置文件nginx.conf 中使用 content_by_lua 或者 cotent_by_lua_file 指令：\n\n1.  content_by_lua 一般在很简单的lua脚本时使用\n\n```javascript\n        location /lua {\n                set $test \"hello, world.\";\n                content_by_lua '\n                        ngx.header.content_type = \"text/plain\";\n                        ngx.say(ngx.var.test);\n                ';\n        }\n\n```\n\nngx.say 就是打印出hello，world，本来就是c写的\n\n2. cotent_by_lua_file 适应于复杂的 lua 脚本，专门放入一个文件中：\n\n```\nlocalition /lua{\n content_by_lua_file lua/hello.lua\n}\n```\n\n3. lua访问redis\n\n```\nlcoaltion /test_redis{\ncontent_by_lua_file lua/test_redis.lua\n}\n```\n\ncat test_redis.lua\n\n```c\nlocal redis = requre \"resty.redis\"\nlocal red = redis :new()\nred.set_timeout(1000)\nlocal ok,err =red:connect(\"127.0.0.1\",6379)\nif not ok then\n    \tngx.say(\"fail to connect\",err)\n    return\nend\nngx.say(\"result\",ok)\n local res,err=red:get(\"dog\") \n  if not res then\n      ngx.say(\"failed to get doy: \",err)\n   return\n   end\n  if res == ngx.null then  \n     ngx.say(\"dog not found\")\n   return\n   end\n   ngx.say(\"dog: \", res)   \n```\n\n## 3. 访问mysql\n\n```mysql\n[root@localhost lua]# pwd\n/opt/openresty/nginx/lua\n[root@localhost lua]# cat mysql_test.lua\nlocal mysql = require \"resty.mysql\"\nlocal db, err = mysql:new()\n\nif not db then\n        ngx.say(\"failed to instantiate mysql: \", err)\n        return\nend\n\ndb:set_timeout(1000)\n\nlocal ok, err, errno, sqlstate = db:connect{\n        host = \"127.0.0.1\",\n        port = 3306,\n        database = \"ngx_lua\",\n        user = \"root\",\n        password=\"digdeep\",\n        max_packet_size = 1024 * 1024\n}\n\nif not ok then\n        ngx.say(\"failed to connect: \", err, \": \", errno, \" \", sqlstate)\n        return\nend\n\nngx.say(\"connected to mysql.\")\n\nlocal res, err, errno, sqlstate = db:query(\"drop table if exists cats\")\nif not res then\n        ngx.say(\"bad result: \", err, \": \", errno, \": \", sqlstate, \".\")\n        return\nend\n\nres, err, errno, sqlstate = db:query(\"create table cats \" .. \"(id int not null primary key auto_increment, \"\n                                        .. \"name varchar(30))\")\nif not res then\n        ngx.say(\"bad result: \", err, \": \", errno, \": \", sqlstate, \".\")\n        return\nend\n\nngx.say(\"table cats created.\")\n\nres, err, errno, sqlstate = db:query(\"insert into cats(name) \" .. \"values (\\'Bob\\'),(\\'\\'),(null)\")\nif not res then\n        ngx.say(\"bad request: \", err, \": \", errno, \": \", sqlstate, \".\")\n        return\nend\n\nngx.say(res.affected_rows, \" rows inserted into table cats \", \"(last insert id: \", res.insert_id, \")\")\n\nres, err, errno, sqlstate = db:query(\"select * from cats order by id asc\", 10)\nif not res then\n        ngx.say(\"bad result \", err, \": \", errno, \": \", sqlstate, \".\")\n        return\nend\n\nlocal cjson = require \"cjson\"\nngx.say(\"result: \", cjson.encode(res))\n\nlocal ok, err = db:set_keepalive(1000, 100)\nif not ok then\n        ngx.say(\"failed to set keepalive: \", err)\n        return\nend\n\n```\n\n## 4.lua 的 capture 和 capture_multi(子查询)\n\n**capture_multi 是 openresty 一个十分强大的功能**。它能极大的减少前端浏览器发送的http请求的数量，突破了浏览器对于同一个服务器并发请求数量的限制，因为他可以将前端的多个http请求减少为只要一个http请求到nginx，然后nginx使用capture_multi特性，对后端发起多个异步并发请求，然后统一将结果返回给前端。下面看一个例子：\n\n首先在nginx.conf中加入下面的 location 配置，并且配置好 nginx 访问 php 的配置：\n\n```\n        location /capture {\n            content_by_lua_file lua/capture.lua;\n            #access_by_lua_file lua/capture.lua;\n        }\n\n        location ~ \\.php$ {\n            root           html;\n            fastcgi_pass   127.0.0.1:9000;\n            fastcgi_index  index.php;\n            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;\n            include        fastcgi_params;\n        }\n\n```\n\ncapture.lua 的代码如下：\n\n```\n[root@localhost html]# curl localhost/capture\nconnected to mysql.\ntable cats created.\n3 rows inserted into table cats (last insert id: 1)\nresult: [{\"name\":\"Bob\",\"id\":1},{\"name\":\"\",\"id\":2},{\"name\":null,\"id\":3}]\n\nset result: 1\ndog: an animal\n\nhello ngx_lua!!!!\n\nfalse\n200\nnil\n\n```\n\nngx.location.capture_multi{… …} 中的多个异步并发请求可以是 nginx.conf 中配置的 location(比如 /mysql_test, /redis_test, /lua)，也可以不是 location配置的路径，比如 index.php 就不是。index.php 就是一个简单的后台php 脚本。当然也可以是一个 java 实现的后台接口。\n\n## 5.openresty的缓存 lua_shared_dict\n\n定义一个缓存：\n\n在nginx的配置文件 nginx.conf 的 http 端下面加入指令：\n\n```\nlua_shared_dict ngx_cache 128m;\n```\n\n就定义了一个 名称为 ngx_cache 大小为128m的内存用于缓存，`注意该缓存是所有nginx work process所共享的`。\n\n在lua脚本中访问缓存：\n\n```\nlocal ngx_cache = ngx.shared.ngx_cache\nlocal value = ngx_cache:get(key)\n\nlocal succ, err, forcible = ngx_cache:set(key, value, exptime)\n\n```\n\n首先在 nginx.conf的server端中加入：\n\n        location /cache {\n            content_by_lua_file lua/cache.lua;\n        }\n编写cache.lua\n\n```c\n[root@localhost lua]# cat cache.lua\nlocal redis = require \"resty.redis\"\nlocal red = redis:new()\n\nfunction set_to_cache(key, value, exptime)\n        if not exptime then\n                exptime = 0\n        end\n        local ngx_cache = ngx.shared.ngx_cache\n        local succ, err, forcible = ngx_cache:set(key, value, exptime)\n        return succ\nend\n\nfunction get_from_cache(key)\n        local ngx_cache = ngx.shared.ngx_cache;\n        local value = ngx_cache:get(key)\n        if not value then\n                value = get_from_redis(key)\n                set_to_cache(key, value)\n                return value\n        end\n\n        ngx.say(\"get from cache.\")\n        return value\nend\n\nfunction get_from_redis(key)\n        red:set_timeout(1000)\n\n        local ok, err = red:connect(\"127.0.0.1\", 6379)\n        if not ok then\n                ngx.say(\"failed to connect: \", err)\n                return\n        end\n\n        local res, err = red:get(key)\n        if not res then\n                ngx.say(\"failed to get doy: \", err)\n                return ngx.null\n        end\n\n        ngx.say(\"get from redis.\")\n        return res\nend\n\nfunction set_to_redis(key, value)\n        red:set_timeout(1000)\n        local ok, err = red:connect(\"127.0.0.1\", 6379)\n        if not ok then\n                ngx.say(\"failed to connect: \", err)\n                return\n        end\n\n        local ok, err = red:set(key, value)\n        if not ok then\n                ngx.say(\"failed to set to redis: \", err)\n                return\n        end\n        return ok\nend\n\nset_to_redis('dog', \"Bob\")\nlocal rs = get_from_cache('dog')\nngx.say(rs)\n\n```\n\n可以使用 ab 测试一下rps(Requests per second):\n\n ab -n 1000 -c 100 -k http://127.0.0.1/cache\n\n## 6.解决缓存失效风暴 lua-resty-lock\n\n缓存失效风暴是指缓存因为时间过期而失效时，会导致所有的请求都去访问 后台的redis或者mysql，而导致CPU性能即刻增长的现象。所以关键是当缓存失效时，用lock保证只有一个线程去访问后台的redis或者mysql，然后更新缓存。需要使用到 lua-resty-lock 模块的加锁、解锁功能。\n\n首先在nginx.conf 的 http 端下面加入指令：\n\n```\nlua_shared_dict ngx_cache 128m;     # cache\nlua_shared_dict cache_lock 100k;    # lock for cache\n```\n\n然后在nginx.conf的server端中加入：\n\n```\n        location /cache_lock {\n            content_by_lua_file lua/cache_lock.lua;\n        }\n```\n\ncache_lock.lua代码：\n\n```c\n[root@localhost lua]# cat cache_lock.lua\nlocal redis = require \"resty.redis\"\nlocal red = redis:new()\nlocal resty_lock = require \"resty.lock\"\nlocal ngx_cache = ngx.shared.ngx_cache\n\nfunction set_to_cache(key, value, exptime)\n        if not exptime then\n                exptime = 0\n        end\n        local succ, err, forcible = ngx_cache:set(key, value, exptime)\n        return succ\nend\n\nfunction get_from_cache(key)\n        local ngx_cache = ngx.shared.ngx_cache;\n        local value = ngx_cache:get(key)\n        if not value then       -- cache miss\n                local lock = resty_lock:new(\"cache_lock\")\n                local elapsed, err = lock:lock(key)\n                if not elapsed then\n                        return fail(\"failed to acquire the lock: \", err)\n                end\n\n                value = get_from_redis(key)\n                if not value then\n                        local ok, err = lock:unlock()\n                        if not ok then\n                                return fail(\"failed to unlock: \", err)\n                        end\n                        ngx.say(\"no value found\")\n                        return\n                end\n\n                local ok, err = ngx_cache:set(key, value, 1)\n                if not ok then\n                        local ok, err = lock:unlock()\n                        if not ok then\n                                return fail(\"failed to unlock: \", err)\n                        end\n                        return faile(\"failed to update ngx_cache: \", err)\n                end\n\n                local ok, err = lock:unlock()\n                if not ok then\n                        return faile(\"failed to unlock: \", err)\n                end\n\n                return value\n        end\n\n        ngx.say(\"get from cache.\")\n        return value\nend\n\nfunction get_from_redis(key)\n        red:set_timeout(1000)\n\n        local ok, err = red:connect(\"127.0.0.1\", 6379)\n        if not ok then\n                ngx.say(\"failed to connect: \", err)\n                return\n        end\n\n        local res, err = red:get(key)\n        if not res then\n                ngx.say(\"failed to get doy: \", err)\n                return ngx.null\n        end\n\n        ngx.say(\"get from redis.\")\n        return res\nend\n\nfunction set_to_redis(key, value)\n        red:set_timeout(1000)\n        local ok, err = red:connect(\"127.0.0.1\", 6379)\n        if not ok then\n                ngx.say(\"failed to connect: \", err)\n                return\n        end\n\n        local ok, err = red:set(key, value)\n        if not ok then\n                ngx.say(\"failed to set to redis: \", err)\n                return\n        end\n        return ok\nend\n\nset_to_redis('dog', \"Bob\")\nlocal rs = get_from_cache('dog')\nngx.say(rs)\n```\n\n","tags":["nginx"],"categories":["nginx"]},{"title":"nginx学习笔记（—）","url":"%2F2020%2F03%2F24%2Fnginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E2%80%94%EF%BC%89.html","content":"    \n\n一个好人加入了战争，恶魔将会逃跑，上帝也会留下眼泪\n\n1.nginx主要学习的是配置文件\n\n首先说下正向代理与反向代理\n\n正向代理：比如我们想要访问国外的网站，我们不知道怎么到达，可以让代理服务器帮助，此时服务器只清楚请求来自哪个代理服务器，而不清楚具体请求者，而我们很明确的知道要访问谁\n\n反向代理：就是分布式，我们访问具体的网站却不知具体的服务器以为被代理到分布的一台机器上啦\n\n    #运行用户\n    user nobody;\n    #启动进程,通常设置成和cpu的数量相等\n    worker_processes  1;\n    >>nginx 主要核心模块俗称的main区\n    #全局错误日志及PID文件\n    #error_log  logs/error.log;\n    #error_log  logs/error.log  notice;\n    #error_log  logs/error.log  info;\n    \n    #pid        logs/nginx.pid;\n    \n    #工作模式及连接数上限\n    events { #events区\n        #epoll是多路复用IO(I/O Multiplexing)中的一种方式,\n        #仅用于linux2.6以上内核,可以大大提高nginx的性能\n        use   epoll; \n    \n        #单个后台worker process进程的最大并发链接数    \n        worker_connections  1024;\n    \n        # 并发总数是 worker_processes 和 worker_connections 的乘积\n        # 即 max_clients = worker_processes * worker_connections\n        # 在设置了反向代理的情况下，max_clients = worker_processes * worker_connections / 4  为什么\n        # 为什么上面反向代理要除以4，应该说是一个经验值\n        # 根据以上条件，正常情况下的Nginx Server可以应付的最大连接数为：4 * 8000 = 32000\n        #说明这里的8000指线程 4是进程\n        # worker_connections 值的设置跟物理内存大小有关\n        # 因为并发受IO约束，max_clients的值须小于系统可以打开的最大文件数\n        # 而系统可以打开的最大文件数和内存大小成正比，一般1GB内存的机器上可以打开的文件数大约是10万左右\n        # 我们来看看360M内存的VPS可以打开的文件句柄数是多少：\n        # $ cat /proc/sys/fs/file-max\n        # 输出 34336\n        # 32000 < 34336，即并发连接总数小于系统可以打开的文件句柄总数，这样就在操作系统可以承受的范围之内\n        # 所以，worker_connections 的值需根据 worker_processes 进程数目和系统可以打开的最大文件总数进行适当地进行设置\n        # 使得并发总数小于操作系统可以打开的最大文件数目\n        # 其实质也就是根据主机的物理CPU和内存进行配置\n        # 当然，理论上的并发总数可能会和实际有所偏差，因为主机还有其他的工作进程需要消耗系统资源。\n    \t# ulimit -SHn 65535\n    }\n    \n    \n    http {\n        #设定mime类型,类型由mime.type文件定义\n        include    mime.types; >这是引入\n        default_type  application/octet-stream;\n        #设定日志格式\n        log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                          '$status $body_bytes_sent \"$http_referer\" '\n                          '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n    \n        access_log  logs/access.log  main;\n    \n        #sendfile 指令指定 nginx 是否调用 sendfile 函数（zero copy 方式）来输出文件，\n        #对于普通应用，必须设为 on,\n        #如果用来进行下载等应用磁盘IO重负载应用，可设置为 off，\n        #以平衡磁盘与网络I/O处理速度，降低系统的uptime.\n        sendfile     on;\n        #tcp_nopush     on;\n    \n        #连接超时时间\n        #keepalive_timeout  0;\n        keepalive_timeout  65;\n        tcp_nodelay     on;\n    \n        #开启gzip压缩\n        gzip  on;\n        gzip_disable \"MSIE [1-6].\";\n    \n        #设定请求缓冲\n        client_header_buffer_size    128k;\n        large_client_header_buffers  4 128k;\n    \n    \n        #设定虚拟主机配置\n        server {\n            #侦听80端口\n            listen    80;\n            #定义使用 www.nginx.cn访问\n            server_name  www.nginx.cn;\n    \n            #定义服务器的默认网站根目录位置\n            root html;\n    \n            #设定本虚拟主机的访问日志\n            access_log  logs/nginx.access.log  main;\n    \n            #默认请求\n            location / {\n                \n                #定义首页索引文件的名称\n                index index.php index.html index.htm;   \n    \n            }\n    \n            # 定义错误提示页面\n            error_page   500 502 503 504 /50x.html;\n            location = /50x.html {\n            }\n    \n            #静态文件，nginx自己处理\n            location ~ ^/(images|javascript|js|css|flash|media|static)/ {\n                \n                #过期30天，静态文件不怎么更新，过期可以设大一点，\n                #如果频繁更新，则可以设置得小一点。\n                expires 30d;\n            }\n    \n            #PHP 脚本请求全部转发到 FastCGI处理. 使用FastCGI默认配置.\n            location ~ .php$ {\n                fastcgi_pass 127.0.0.1:9000;\n                fastcgi_index index.php;\n                fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;\n                include fastcgi_params;\n            }\n    \n            #禁止访问 .htxxx 文件\n                location ~ /.ht {\n                deny all;\n            }\n    \n        }\n    }\n\n机会总是留给那些有准备的人\n\n2.nginx命令\n\nnginx命令行参数\n\n  命令             \t说明                                      \n  -p prefix      \t设置 nginx prefix 路径，这是存储 nginx 服务文件的路径，默认为 /etc/nginx\n  -c filename    \t不使用默认配置文件，使用指定的配置文件(default: /etc/nginx/nginx.conf)\n  nginx          \t启动命令                                    \n  nginx -s stop  \t快速关闭 nginx                              \n  nginx -s quit  \t优雅的关闭 nginx                             \n  nginx -s reload\t重新加载配置                                  \n  nginx -s reopen\t重新打开日志文件                                \n  nginx  -t      \t检查配置文件                                  \n\n关于关闭进程补充一下linux进程知识 以nginx为例\n\nps -ef |grep nginx 找到进程然后 kill -9 进程号\n\nnetstat -ntlp  #查看当前所有tcp端口·\n\nnetstat -ntulp |grep 80 #查所有80端口的使用情况\n\nps aux | egrep '(PID|nginx)' kill -15 pid #中止\n\n也可以临时指定，支持热部署\n\n总之岁月漫长，然而值得等待\n\n3.upstream负载均衡块\n\nNginx的 HttpUpstreamModule提供对后端（backend）服务器的简单负载均衡。一个最简单的 upstream 写法如下：\n\n      upstream backend {\n        server backend1.example.com;\n        server backend2.example.com;\n        server backend3.example.com;\n    }\n    \n    server {\n        location / {\n            proxy_pass http://backend;\n        }\n    }\n    \n\n1. 通过 upstream 可以设定后端服务器，指定的方式可以是 IP 地址与端口、域名、UNIX 套接字（socket）。其中如果域名可以被解析为多个地址，则这些地址都作为 backend。下面举例说明\n\n     upstream backend {\n        server blog.csdn.net/poechant;\n        server 145.223.156.89:8090;\n        server unix:/tmp/backend3;\n    }\n\n2. 负载均衡策略\n   默认情况下，Nginx 会为你提供轮询作为负载均衡策略。但是这并不一定能够让你满意。比如，某一时段内的一连串访问都是由同一个用户 Michael 发起的，那么第一次 Michael 的请求可能是 backend2，\n   而下一次是 backend3，然后是 backend1、backend2、backend3…… 在大多数应用场景中，这样并不高效。当然，也正因如此，\n   Nginx 为你提供了一个按照 Michael、Jason、David 等等这些乱七八糟的用户的 IP 来 hash 的方式，\n   这样每个 client 的访问请求都会被甩给同一个后端服务器。具体的使用方式如下：\n    \n\n    upstream backend {\n        server blog.csdn.net/poechant down;\n        server 145.223.156.89:8090;\n        server unix:/tmp/backend3;\n    } \n\n还可以使用指定权重（weight）的方式，如下：\n\n     upstream backend {\n        server backend1.example.com;\n        server 123.321.123.321:456 weight=4;\n    } \n\n默认情况下 weight 为 1，对于上面的例子，第一个 server 的权重取默认值 1，第二个是 4，所以相当于第一个 server 接收 20% 的请求，第二接收 80% 的。要注意的是 weight 与 ip_hash 是不能同时使用的，原因很简单，他们是不同且彼此冲突的策略\n\nurl_hash（第三方）\n\n按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。\n\n注意：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法。\n\n    upstream resinserver{ \n          server 10.0.0.10:7777; \n          server 10.0.0.11:8888; \n          hash $request_uri; \n          hash_method crc32; \n    }\n\npstream还可以为每个设备设置状态值，这些状态值的含义分别如下：\n\ndown 表示单前的server暂时不参与负载.\n\nweight 默认为1.weight越大，负载的权重就越大。\n\nmax_fails ：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream 模块定义的错误.\n\nfail_timeout : max_fails次失败后，暂停的时间。\n\nbackup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。\n\n3. 重试策略\n\n可以为每个 backend 指定最大的重试次数，和重试时间间隔。所使用的关键字是 max_fails 和 fail_timeout。如下所示：\n\n    upstream backend {\n    server backend1.example.com weight=5;\n    server 54.244.56.3:8081 max_fails=3 fail_timeout=30s;\n    } \n\n在上例中，最大失败次数为 3，也就是最多进行 3 次尝试，且超时时间为 30秒。max_fails 的默认值为 1，fail_timeout 的默认值是 10s。传输失败的情形，由 proxy_next_upstream 或 fastcgi_next_upstream 指定。而且可以使用 proxy_connect_timeout 和 proxy_read_timeout 控制 upstream 响应时间。\n\n有一种情况需要注意，就是 upstream 中只有一个 server 时，max_fails 和 fail_timeout 参数可能不会起作用。导致的问题就是 nginx 只会尝试一次 upstream 请求，如果失败这个请求就被抛弃了 : ( ……解决的方法，比较取巧，就是在 upstream 中将你这个可怜的唯一 server 多写几次，如下：\n\n    upstream backend {\n    server backend.example.com max_fails fail_timeout=30s;\n    server backend.example.com max_fails fail_timeout=30s;\n    server backend.example.com max_fails fail_timeout=30s;\n    } \n\n4. 备机策略\n\n从 Nginx 的 0.6.7 版本开始，可以使用“backup”关键字。当所有的非备机（non-backup）都宕机（down）或者繁忙（busy）的时候，就只使用由 backup 标注的备机。必须要注意的是，backup 不能和 ip_hash 关键字一起使用。举例如下：\n\n    upstream backend {\n    server backend1.example.com;\n    server backend2.example.com backup;\n    server backend3.example.com;\n    }\n\n4.localtion指令详解\n\n    Nginx的HTTP配置主要包括三个区块，结构如下：\n    http { //这个是协议级别\n    　　include mime.types;\n    　　default_type application/octet-stream;\n    　　keepalive_timeout 65;\n    　　gzip on;\n    　　　　server { //这个是服务器级别\n    　　　　　　listen 80;\n    　　　　　　server_name localhost;\n    　　　　　　　　location / { //这个是请求级别\n    　　　　　　　　　　root html;\n    　　　　　　　　　　index index.html index.htm;\n    　　　　　　　　}\n    　　　　　　}\n    }\n\nlocation区段\n\n通过指定模式来与客户端请求的URI相匹配，基本语法如下：location [=|~|~*|^~|@] pattern{……}\n\n1、没有修饰符 表示：必须以指定模式开始，如：\n\n    server {\n    　　server_name baidu.com;\n    　　location /abc {\n    　　　　……\n    　　}\n    }\n    \n    \n    那么，如下是对的：\n    http://baidu.com/abc\n    http://baidu.com/abc?p1\n    http://baidu.com/abc/\n    http://baidu.com/abcde\n\n2、=表示：必须与指定的模式精确匹配\n\n    server {\n    server_name sish\n    　　location = /abc {\n    　　　　……\n    　　}\n    }\n    那么，如下是对的：\n    http://baidu.com/abc\n    http://baidu.com/abc?p1\n    如下是错的：\n    http://baidu.com/abc/\n    http://baidu.com/abcde\n\n3、~ 表示：指定的正则表达式要区分大小写\n\n    server {\n    server_name baidu.com;\n    　　location ~ ^/abc$ {\n    　　　　……\n    　　}\n    }\n    那么，如下是对的：\n    http://baidu.com/abc\n    http://baidu.com/abc?p1=11&p2=22\n    如下是错的：\n    http://baidu.com/ABC\n    http://baidu.com/abc/\n    http://baidu.com/abcde\n\n4、~* 表示：指定的正则表达式不区分大小写\n\n    server {\n    server_name baidu.com;\n    location ~* ^/abc$ {\n    　　　　……\n    　　}\n    }\n    那么，如下是对的：\n    http://baidu.com/abc\n    http://baidu..com/ABC\n    http://baidu..com/abc?p1=11&p2=22\n    如下是错的：\n    http://baidu..com/abc/\n    http://baidu..com/abcde\n\n5、^~ 类似于无修饰符的行为，也是以指定模式开始，不同的是，如果模式匹配，\n那么就停止搜索其他模式了。\n6、@ ：定义命名location区段，这些区段客户段不能访问，只可以由内部产生的请\n求来访问，如try_files或error_page等\n\n    location  = / {\n      # 只匹配请求 \"/\"\n      [ configuration A ] \n    }\n    location  / {\n      # 匹配任何请求，因为所有请求都是以\"/\"开始\n      # 但是更长字符匹配或者正则表达式匹配会优先匹配\n      [ configuration B ] \n    }\n    location /documents/ {\n      # 匹配所有 /documents/ 开头的请求，在没有正则表达\n      # 式匹配时选择该locaiton\n      [ configuration C ]\n    }\n    location ^~ /images/ {\n      # 匹配任何以 /images/ 开始的请求，并停止匹配其它location\n      [ configuration D ] \n    }E\n    location ~* .(gif|jpg|jpeg)$ {\n      # 匹配以 gif, jpg, or jpeg结尾的请求. \n      # 但是所有 /images/ 目录的请求将由 [Configuration D]处理.   \n      [ configuration E ] \n    }\n    请求URI例子:\n    \n    / -> 匹配A\n    /index.html -> 匹配B\n    /documents/a.html -> 匹配C\n    /images/1.gif -> 匹配D\n    /documents/1.jpg -> 匹配E\n\n查找顺序和优先级\n1：带有“=“的精确匹配优先\n2：没有修饰符的精确匹配\n3：正则表达式按照他们在配置文件中定义的顺序\n4：带有“^~”修饰符的，开头匹配\n5：带有“~” 或“~*” 修饰符的，如果正则表达式与URI匹配\n6：没有修饰符的，如果指定字符串与URI开头匹配\n\nroot 、alias指令区别\n\n    location /img/ {\n        alias /var/www/image/;\n    }\n\n    #若按照上述配置的话，则访问/img/目录里面的文件时，ningx会自动去/var/www/image/目录找文件\n\n    location /img/ {\n        root /var/www/image;\n    }\n\n    若按照这种配置的话，则访问/img/目录下的文件时，nginx会去/var/www/image/img/目录下找文件。] \n\nalias是一个目录别名的定义，root则是最上层目录的定义。\n\n还有一个重要的区别是alias后面必须要用“/”结束，否则会找不到文件的。。。而root则可有可无~~\n","tags":["nginx"],"categories":["nginx"]},{"title":"春来","url":"%2F2020%2F03%2F24%2F%E6%98%A5%E6%9D%A5.html","content":"北方大雪纷飞，南国大雨连绵。果真是，往昔不可追，时令不可违。\n\n《道德经》有句：“飘风不终朝，骤雨不终日。”天地万物皆有其规律，开落有序，荣枯有定。当下的雨雪，以及灾事，亦是不能久长。\n\n“春日迟迟，采蘩祁祁。”这个春天，虽然来得缓慢，却会如约而至。且比之从前任何一个春天，更加令人期待，温暖有情。\n\n那时的人间，杏雨梨云，群莺乱飞。满目春光，无人做主。而我，也该归去梅庄，剪枝插瓶，春水煎茶。\n\n封城已有半月余，等待成了信仰。这一年，许多人的命运被改写，但都学会了坦然以待。而我，有一壶酒，一盏茶，足矣消闷解烦。任凭世事飞沙走石，我自是风雨不动，慌乱不惊。\n\n“蜉蝣之羽，衣裳楚楚。心之忧矣，于我归处？”说不劳神忧心，是虚话。想当年杜工部忧国忧民，生出“安得广厦千万间，大庇天下寒士俱欢颜”之叹。我虽薄弱，却亦有此心。\n\n太湖水畔的山庄，也为此搁浅，等候时光给予美妙的安排。往后余生，归去林泉，种菜伺花，无可更改。愿有一畦菜地，供养苍生，几片瓦当，庇护万民。\n\n寸阴可贵，一刻千金。静下来的时候，许多人对生命有了新的认知，对未知的将来，重新有了念想。不再执迷，亦不抱怨，因为浩浩荡荡的一辈子，有无数的风雨消磨，我们要活得心安境宽，有情有味。\n\n无论光阴几何，人的一生，终要做一件比生命，更为宏伟的事业。过些时日，我又该生火炙茶，采花酿酒，读书养气，写字谋生。\n\n世上的人，都有来时，亦有归处。有人愿化身石桥，受五百年的风吹日晒雨淋，只为一人打桥上经过。\n\n而我，愿做驿外断桥边的一株白梅，若人世的一剪清光，照彻凡尘每一个悲伤、迷惘的路人。\n","tags":["美文"],"categories":["心情"]}]